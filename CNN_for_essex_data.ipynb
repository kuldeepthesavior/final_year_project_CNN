{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is CNN to train and test the loaded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the required library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import imutils\n",
    "import logging\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deciding the features and Epochs to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_FEATURES = 512\n",
    "BATCH_SIZE = 23\n",
    "BATCHES_IN_EPOCH = 140\n",
    "TRAIN_SIZE = BATCHES_IN_EPOCH * BATCH_SIZE\n",
    "TEST_SIZE = 692\n",
    "NUMBER_OF_EPOCHS =10\n",
    "NUMBER_OF_EXPERIMENTS = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting the train test labels to numeric values using OneHotEncoding  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(value):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    train_labels_values = list(value)\n",
    "    # integer encode\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(train_labels_values)\n",
    "    \n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    return onehot_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading pickle file of labels from Loading_images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1384\n",
      "3220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hindu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "/home/hindu/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "data=pickle.load(open(\"Train_test_data.npy\",\"rb\"))\n",
    "Train_images=data[\"Train\"][0]\n",
    "Train_labels=data[\"Train\"][1]\n",
    "Test_images=data[\"Test\"][0]\n",
    "Test_labels=np.array(data[\"Test\"][1])\n",
    "\n",
    "Train_labels_one_hot=one_hot_encode(Train_labels)\n",
    "Test_labels_one_hot=one_hot_encode(Test_labels)\n",
    "\n",
    "print(len(Test_labels))\n",
    "print(len(Train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape,name):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1,name=name) ## using normal distribution of tensorflow\n",
    "    return tf.Variable(initial) ## storing in tensorflow variable to upudate further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_variable(shape,name):\n",
    "    initial = tf.constant(0.1, shape=shape,name=name) ## Initialing with contant 0.1\n",
    "    return tf.Variable(initial) # storing in variable to upudate further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the 2 dimentional convolution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x, W,name):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME',name=name) ## stride is the steps jumped by filter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the maxpooling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool_2x2(x,name):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME',name=name) ## ksize is size of the window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the batches to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(images,labels,epochs):\n",
    "    l = len(images)\n",
    "    for _ in range(epochs):\n",
    "        for ndx in range(0, l, BATCH_SIZE):\n",
    "            yield [images[ndx:min(l,ndx+BATCH_SIZE)],labels[ndx:min(l,ndx+BATCH_SIZE)]]\n",
    "\n",
    "## this is generater function os python           \n",
    "## this will generate a batch (list) of images(arrays) and labels which will contain rows = BATCH_SIZE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using the logs to keep the flow of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger=logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the convolution network function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvNet(number_of_training_epochs):\n",
    "    print(\"\\n#########################\\nConvNet Train/Test\\n#########################\")\n",
    "    epoch_acc=[]    ## to  record th accuracy in each epoch\n",
    "    temp_epoch_acc=[]\n",
    "    initial_time = time.time()  ## to note start time\n",
    "    epoch_start_time=time.time() ## to note the start time of each epoch\n",
    "    batch_gen=batch_generator(Train_images,Train_labels_one_hot,number_of_training_epochs) ## getting the batches\n",
    "    print(\"___starting the training of the data_______\")\n",
    "    for i in range(number_of_training_epochs * BATCHES_IN_EPOCH):\n",
    "        batch = next(batch_gen)\n",
    "        temp_label=np.array(batch[1])\n",
    "        train_accuracy = model_accuracy.eval(feed_dict={x: batch[0], y_: temp_label, keep_prob: 1.0})\n",
    "        temp_epoch_acc.append(train_accuracy)     \n",
    "        if i%140==0 and i>=1:\n",
    "            avg=sum(temp_epoch_acc)/len(temp_epoch_acc)\n",
    "            temp_epoch_acc=[]\n",
    "            epoch_acc.append(avg)\n",
    "            epoch_end_time=time.time()-epoch_start_time\n",
    "            epoch_start_time=time.time()\n",
    "            print(\"__\"+\"Epoch [\"+str(i//BATCHES_IN_EPOCH)+\"/\"+str(NUMBER_OF_EPOCHS)+\n",
    "                  \"]\\n Accuracy:  \"+str(avg)+\" , Time: \"+ str(round(epoch_end_time))+\"s \")\n",
    "        train_step.run(feed_dict={x: batch[0], y_: temp_label, keep_prob: 0.5})\n",
    "       \n",
    "    training_time = time.time()-initial_time  ## Calculating the training time\n",
    "    print(\"\\nTraining Time = \", training_time)\n",
    "     \n",
    "        \n",
    "    ## starting the tensorflow session\n",
    "    accuracy=sess.run([model_accuracy],feed_dict={x: Test_images[:500],y_:Test_labels_one_hot[:500] ,keep_prob: 1.0})\n",
    "    \n",
    "    print(\"\\nConvNet accuracy =\", accuracy)\n",
    "    return accuracy, training_time,epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the assigning all the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#########################\n",
      "Starting\n",
      "#########################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hindu/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n#########################\\nStarting\\n#########################\\n\")\n",
    "\n",
    "config=tf.ConfigProto(log_device_placement=True, device_count={'GPU':0})  \n",
    "\n",
    "## log_device_placement prints out device information\n",
    "\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9   ## allocating 90% GPU to tensorflow to train\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession(config=config) ## starting the tensorflow session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#########################\n",
      "Building ConvNet\n",
      "#########################\n",
      "\n",
      "#########################\n",
      "ConvNet Train/Test\n",
      "#########################\n",
      "___starting the training of the data_______\n",
      "__Epoch [1/10]\n",
      " Accuracy:  0.09374036383649982 , Time: 17s \n",
      "__Epoch [2/10]\n",
      " Accuracy:  0.31645962284611806 , Time: 17s \n",
      "__Epoch [3/10]\n",
      " Accuracy:  0.48105589726141523 , Time: 17s \n",
      "__Epoch [4/10]\n",
      " Accuracy:  0.5996894400034632 , Time: 17s \n",
      "__Epoch [5/10]\n",
      " Accuracy:  0.6826086940509932 , Time: 18s \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n#########################\\nBuilding ConvNet\\n#########################\")\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 1760])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 115])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,44,40,1])\n",
    "\n",
    "## first layer\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32],\"first_weight\")                ## initial weights\n",
    "b_conv1 = bias_variable([32],\"first_bias\")                             ## initial bais\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1,\"first_conv\") + b_conv1)  ## first convolution  y =W*X + B\n",
    "h_pool1 = max_pool_2x2(h_conv1,\"first_pool\")                           ## maxpooling\n",
    "\n",
    "## second layer\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64],\"second_weight\")              ## second weights\n",
    "b_conv2 = bias_variable([64],\"second_bias\")                            ## second bias\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2,\"second_conv\") + b_conv2) ## second convolution  y =W*X + B\n",
    "h_pool2 = max_pool_2x2(h_conv2,\"second_pool\")                          ## maxpooling\n",
    "\n",
    "\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 11*10*64])    ## Flattening \n",
    "\n",
    "W_fc1 = weight_variable([11 * 10 * 64, NUMBER_OF_FEATURES],\"third_weight\")\n",
    "b_fc1 = bias_variable([NUMBER_OF_FEATURES],\"third_bias\")\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)      ## activation function RELU (max(0,x))\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)   ## drop out layer\n",
    "\n",
    "W_fc2 = weight_variable([NUMBER_OF_FEATURES, 115],\"last_weight\")\n",
    "b_fc2 = bias_variable([115],\"last_bias\")\n",
    "dl2=tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n",
    "y_conv=tf.nn.softmax(dl2)                                      ## activation function SOFTMAX (exp(x)/sum(exp(x)))\n",
    "\n",
    "\n",
    "\n",
    "## calculating the cross entropy\n",
    "cross_entropy=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=dl2, labels=y_)) \n",
    "\n",
    "## optimizing the model with AdamOptimizer\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(cross_entropy)\n",
    "\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1),tf.argmax(y_,1))   ## comparing with predicted result to labels\n",
    "model_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  ## checking the accuracy of model\n",
    "\n",
    "tf.summary.scalar(\"cross\",model_accuracy)\n",
    "merged_summary=tf.summary.merge_all()\n",
    "train_writer=tf.summary.FileWriter('graphs',sess.graph)\n",
    "\n",
    "\n",
    "sess.run(tf.global_variables_initializer())    ## initialing the tensorflow variables\n",
    "test_acc,training_time,epoch_acc_list=ConvNet(NUMBER_OF_EPOCHS) ## traing the NET\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
